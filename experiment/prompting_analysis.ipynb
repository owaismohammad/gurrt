{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215706a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only one traversal to clean the response, and it also converts the text to lowercase\n",
    "import re\n",
    "\n",
    "def clean_response(text):\n",
    "    return re.sub(r\"^\\s+|\\s+$|\\*\\*|[\\r\\n\\t]+|\\s+\", \" \", text).lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdcfe042",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=r'C:\\Users\\fareh\\coding\\video-rag\\sample-1.png'\n",
    "img2=r'C:\\Users\\fareh\\coding\\video-rag\\sample-2.png'\n",
    "img3=r'C:\\Users\\fareh\\coding\\video-rag\\sample-3.png'\n",
    "img4=r'C:\\Users\\fareh\\coding\\video-rag\\sample-4.png'\n",
    "img5=r'C:\\Users\\fareh\\coding\\video-rag\\sample-5.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d316ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import  chat\n",
    "def generate_caption():\n",
    "   \n",
    "    response = chat(\n",
    "    model='qwen3-vl:2b',\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\":'''\n",
    "you are image captoning agennt , caption all the image sent to you one by one '''\n",
    "    },\n",
    "\n",
    "    {\n",
    "    'role': 'user',\n",
    "    'content': 'What is in this image? .',\n",
    "    'images': [img1,img2,img3,img4,img5],\n",
    "    }\n",
    "    ],\n",
    "    )\n",
    "    print(response.message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df3c2575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image consists of two distinct visual components: a **Python code snippet** from a Jupyter Notebook environment and a **visual diagram** of an AI/ML interface for a vision-related model. Here's a detailed breakdown:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Python Code Snippet (Top-Left Section)**\n",
      "This is a code block displayed within a Jupyter Notebook interface. It contains:\n",
      "- **Environment setup**:  \n",
      "  - A line to set an environment variable `HF_TOKEN` (likely for Hugging Face's token management).  \n",
      "  - Imports of `AutoTokenizer` and `AutoModelForCausalLM` from the `transformers` library (used for handling large language models).  \n",
      "- **Model configuration**:  \n",
      "  - A reference to a model named `google/gemma-3-4b-it` (a GEMMA 3.4 billion parameter model).  \n",
      "- **Code for model interaction**:  \n",
      "  - A function `response = chat(...)` that processes messages and generates responses.  \n",
      "  - Key logic includes:  \n",
      "    - Defining `messages` with fields like `role`, `content`, and `images` (for input/output handling).  \n",
      "    - Using `print(response.message.content)` to output structured responses.  \n",
      "- **Warnings**:  \n",
      "  - Errors or warnings indicating missing packages (e.g., `TqdmWarning: IProgress not found` and prompts to update Jupyter-related dependencies).  \n",
      "\n",
      "This snippet illustrates how a Python application configures a model (Gemma-3) for text/image processing tasks, with focus on structure of inputs/outputs.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Visual Diagram (Bottom-Right Section)**\n",
      "This is a sidebar interface with a **neural network architecture diagram** for a vision model (likely related to the `gemma` model). The diagram includes:  \n",
      "- **Labeled components**:  \n",
      "  - Nodes like `Layer #1` (indicating a foundational processing layer).  \n",
      "  - Labels for key features: *Streaming*, *Thinking*, *Structured Outputs*, and *Web search*.  \n",
      "- **Handwritten notes**:  \n",
      "  - A note in the top-right corner: *“What is in this image? Be concise.”*  \n",
      "  - Another handwritten entry: *“images”* (referencing an input/output structure).  \n",
      "- **Equations**:  \n",
      "  - Simple math expressions like `6+2=8` (likely a mock example of the model's internal logic).  \n",
      "\n",
      "This visual combines a functional code block (from the Jupyter Notebook) with a diagram of how the model operates, emphasizing its capabilities (e.g., image processing) and input/output structure.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Context**\n",
      "- The **code snippets** are part of a **Jupyter Notebook project** aimed at implementing a vision-based AI system (involving a GEMMA model for tasks like image understanding or object detection).  \n",
      "- The **diagram** and **code** together illustrate how the system processes images:  \n",
      "  - Input: An image (or multiple images) via the `images` field.  \n",
      "  - Output: A structured response (e.g., text output) based on the model's logic.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Purpose**\n",
      "The images serve as a **technical reference** for developers working with a vision-based AI model (Gemma-3), demonstrating how to:  \n",
      "- Set up the model with required imports.  \n",
      "- Structure input/output for a vision task.  \n",
      "- Understand the model's operational flow (even though the exact architecture is not fully visible due to a note).  \n",
      "\n",
      "This setup is typical for tasks like image classification, object detection, or generating descriptive text from images.\n"
     ]
    }
   ],
   "source": [
    "generate_caption()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2918bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys1='''\n",
    "You are an image analysis assistant.\n",
    "\n",
    "Describe the image using the following structure:\n",
    "Objects: list the important objects.\n",
    "Actions: what is happening.\n",
    "Scene: environment or location.\n",
    "Details: any additional useful visual information.\n",
    "\n",
    "Be concise but informative. Only describe what is visible.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "726414ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticSerializationError",
     "evalue": "Error calling function `serialize_model`: ValueError: File C:\\Users\\fareh\\coding\\video-rag\\image.png does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPydanticSerializationError\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mgenerate_caption\u001b[39m\u001b[34m(system_prompt, path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_caption\u001b[39m(system_prompt,path):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mqwen3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWhat is in this image? .\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(clean_response(response.message.content))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\ollama\\_client.py:380\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, logprobs, top_logprobs, format, options, keep_alive)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    319\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    320\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    331\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    332\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    334\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m    365\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m    366\u001b[39m     ChatResponse,\n\u001b[32m    367\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    368\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m/api/chat\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    369\u001b[39m     json=\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m      \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[32m    381\u001b[39m     stream=stream,\n\u001b[32m    382\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\pydantic\\main.py:464\u001b[39m, in \u001b[36mBaseModel.model_dump\u001b[39m\u001b[34m(self, mode, include, exclude, context, by_alias, exclude_unset, exclude_defaults, exclude_none, exclude_computed_fields, round_trip, warnings, fallback, serialize_as_any)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmodel_dump\u001b[39m(\n\u001b[32m    419\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    420\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    433\u001b[39m     serialize_as_any: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    434\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    435\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"!!! abstract \"Usage Documentation\"\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[33;03m        [`model_dump`](../concepts/serialization.md#python-mode)\u001b[39;00m\n\u001b[32m    437\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    462\u001b[39m \u001b[33;03m        A dictionary representation of the model.\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_serializer__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_unset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_unset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_defaults\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_defaults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_computed_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_computed_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m        \u001b[49m\u001b[43mround_trip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mround_trip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwarnings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarnings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserialize_as_any\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserialize_as_any\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mPydanticSerializationError\u001b[39m: Error calling function `serialize_model`: ValueError: File C:\\Users\\fareh\\coding\\video-rag\\image.png does not exist"
     ]
    }
   ],
   "source": [
    "generate_caption(sys1,img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d7ac3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objects: neural network diagram (nodes with labels like $b_{11}$, $b_{12}$), table of numerical values, handwritten annotations (e.g., \"2.6\", \"6+2=8\"), interface elements (menu tabs, toolbar icons). actions: diagramming forward propagation process for a neural network, including layer connections and calculation of trainable parameters. scene: digital screen of a software interface (likely a tutorial for machine learning). details: date/time \"03 march 2022 06:07\"; labels \"d9 forward propagation\"; table columns labeled \"c gpa\", \"i 10th m\", \"12th m\", \"placed\"; equation \"prediction → σ(wx + b)\" and handwritten notes like \"layer #1\".\n"
     ]
    }
   ],
   "source": [
    "generate_caption(sys1,img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0fc0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys2='''\n",
    "You are a visual reasoning assistant.\n",
    "When given an image, first describe the important objects,\n",
    "then reason step by step, and finally give a concise answer.\n",
    "Do not hallucinate details that are not visible.\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d83aa9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### description of important objects: 1.  neural network diagram : the image shows a forward propagation diagram of a neural network with three layers (input layer, hidden layer, output layer). -  input layer : contains nodes \\(x_{i1}\\), \\(x_{i2}\\), \\(x_{i3}\\), etc. -  hidden layer : nodes labeled \\(b_{11}\\), \\(b_{12}\\), \\(b_{13}\\) (with connections to the output layer). -  output layer : a single node for prediction. -  weights and biases : illustrated with handwritten notations like \\(w_{11}\\), \\(w_{12}\\), \\(w_{13}\\), and \\(b_{11}\\), \\(b_{12}\\), \\(b_{13}\\) (e.g., \\(w_{11}\\) is highlighted in yellow). 2.  handwritten notes and equations : - a table labeled “# of trainable parameters” with rows like “c2pa” and columns for numbers (e.g., 7.2, 72, 81) and “placed.” - mathematical formulas: - “prediction → σ(w^t x + b)” (where \\(σ\\) is the sigmoid function). - example calculations: “6 + 2 = 8”, “2 + 1/3”, and “3 + 3/2.” - annotations like “2.6” (in red) and “layer #1” for structural labeling. 3.  interface elements : - a purple header bar with tabs “home,” “insert,” “draw,” “view.” - a “stop” button and a note about exiting full screen. - a timestamp: “03 march 2022, 06:07.” ### reasoning steps: 1. the image is a visual explanation of a neural network’s forward propagation process, using handwritten notes to illustrate layer connections, weight values, and activation function rules. 2. key components are the neural network structure (input → hidden → output layers), handwritten mathematical formulas for computation, and a table summarizing the number of trainable parameters. 3. the layout emphasizes a step-by-step breakdown of how data flows through the network to produce a prediction. ### concise answer: the image displays a diagram of a neural network’s forward propagation process, with handwritten notes explaining layer connections, weight values (e.g., \\(w_{11}\\)), and activation function calculations. it includes a table for “# of trainable parameters,” formulas for prediction (\\(σ(w^t x + b)\\)), and visual annotations of nodes and computations. the interface is a screenshot from a tool labeled “d9 forward propagation,” with navigation tabs for home, insert, draw, and view.\n"
     ]
    }
   ],
   "source": [
    "generate_caption(sys2,img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f97e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here's a description of the image and a step-by-step reasoning to answer your question:  description of objects:  the image shows a diagram illustrating the process of forward propagation in a neural network. it includes: *  neural network layers:  two layers are clearly depicted: \"layer #1\" and the subsequent layer. *  weights and biases:  various weights (w₁, w₂, w₃, w’₂, w’₃) and biases (b₁, b₂, b₃) are represented with numbers. *  activation functions:  sigmoid activation functions (represented by “σ”) are indicated at several points. *  arrows:  arrows show the flow of data and calculations between layers. *  output:  the final output of the network is shown as an equation: y = 0 + *  calculations:  some calculations are shown as equations (e.g., 6+2=8, 3/1).  reasoning:  1.  forward propagation:  the diagram depicts the forward pass of a neural network. this means the input data is fed through the network, and calculations are performed at each layer. 2.  layer 1:  the first layer (layer #1) takes inputs x₁, x₂, and x₃. it multiplies these inputs by weights and adds biases. 3.  activation:  the result of each layer is then passed through a sigmoid activation function (σ). 4.  output:  the final layer outputs 'y'.  concise answer:  the image illustrates the forward propagation process of a neural network, showing the flow of data through layers with weights, biases, and sigmoid activation functions, ultimately producing an output 'y'.\n"
     ]
    }
   ],
   "source": [
    "generate_caption(sys2,img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace88c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here’s a description of the image and my reasoning:  objects:  *  car:  a light blue and white vintage volkswagen beetle is parked on a sidewalk. *  wall:  the car is parked in front of a yellow wall. *  doors:  there are two dark wooden doors set into the wall.  reasoning:  1. the most prominent object is a classic volkswagen beetle car. 2. the car is situated on a paved sidewalk. 3. behind the car is a wall painted yellow. 4. two doors are built into the yellow wall.  concise answer:  a light blue volkswagen beetle is parked in front of a yellow wall with two wooden doors.\n"
     ]
    }
   ],
   "source": [
    "generate_caption(sys2,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed3f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here's a description of the image:  objects:  * a man is the primary subject. he is pale, and appears to be expressing concern or frustration, with his mouth open and hands raised. * the man is against a solid black background. * written on the screen in white, is the integral symbol ∫ followed by the expression x-1.  reasoning:  the image captures a moment from a video. the video title suggests it's an educational video on calculus or integration. the symbol written on the screen, ∫x-1, indicates that the video is likely explaining how to calculate the integral of the function x-1.  concise answer:  the image shows a man against a black background with the integral of x-1 written on the screen.\n"
     ]
    }
   ],
   "source": [
    "generate_caption(sys2,img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e64254",
   "metadata": {},
   "source": [
    "without any system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "389d0f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this image displays a  screenshot of a software interface  focused on a  neural network tutorial for \"forward propagation\"  (likely from a course or educational material). here's a breakdown of the key elements: -  software interface : the top bar shows navigation tabs (“home,” “insert,” “draw,” “view”) and a timestamp (“03 march 2022 06:07”). the interface resembles a tool for drawing or illustrating neural networks (possibly a tool for visualizing machine learning concepts). -  neural network diagram : - a  three-layer neural network  is depicted: - *input layer*: features inputs like \\( x_{1i}, x_{2i}, x_{3i} \\) (labeled with red arrows). - *hidden layer*: contains nodes (e.g., \\( b_{11}, b_{12}, b_{13} \\)) with connections to the input layer and output layer. - *output layer*: shows the prediction formula \\( \\sigma(w^t x + b) \\) (where \\( \\sigma \\) is the sigmoid activation function). -  weights (\\( w \\)) and biases (\\( b \\)) : handwritten notes highlight weight values (e.g., \\( w_{11}, w_{12}, w_{13} \\)) and bias values (e.g., \\( b_{11}, b_{12}, b_{13} \\)). the diagram includes handwritten calculations (e.g., “6 + 2 = 8,” “2 × 1 + 2”) to illustrate how weights and biases are used. -  annotations and labels : - text on the top right explains “# of trainable parameters” with a table showing values like “7.2, 72, 69, 81” for different sections. - notes like “\\( c \\), \\( g \\), \\( p \\), \\( i \\), \\( q \\), \\( 10^{th} \\), \\( 12^{th} \\), \\( placed \\)” and a table of values are handwritten. - the diagram uses colored arrows and markings to show the flow of data (e.g., from inputs to hidden layer to output). -  context : the image is a visual guide for understanding how a neural network “propagates” inputs through layers to produce predictions, with an emphasis on forward propagation (the process of moving from input to output without backpropagation). in summary, the image is a  handwritten diagram of a neural network’s forward propagation process , explaining the structure of layers, weights, biases, and example calculations for a specific neural network (likely for educational or tutorial purposes).\n"
     ]
    }
   ],
   "source": [
    "generate_caption(\"\",img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d004250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here's what's in the image: *  a man:  he's a young man with a somewhat distressed expression, gesturing with his hands. *  integral symbol:  the main focus is the integral symbol (∫) drawn on a black background. *  function:  underneath the integral symbol is the function x⁻¹. *  black background:  the background is a solid black color. this image is a screenshot from a video titled “this integral breaks math”.\n"
     ]
    }
   ],
   "source": [
    "generate_caption(\" \",img2)  #bekar response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys3='''\n",
    "You are an assistant that produces dense captions for images.\n",
    "Describe:\n",
    "- all visible objects\n",
    "- interactions between objects\n",
    "- activities\n",
    "- environment\n",
    "- notable visual details\n",
    "Avoid speculation. Provide factual descriptions only.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c37d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is a dense description of the image:  objects:  *  vehicle:  a light teal-colored volkswagen beetle is positioned on the left side of the image. the car has a white stripe running along the top half of its body. it features round headlights, a prominent curved roof, and steel wheels with silver hubcaps. *  wall:  a tall, light yellow wall occupies the background. the wall is constructed of stone or concrete blocks. *  doors:  two wooden doors are visible in the upper right corner of the image. the doors are a dark brown color and have a rectangular shape with evenly spaced, dark wooden vertical slats. each door has a central, dark wooden handle. *  sidewalk:  a gray concrete sidewalk runs along the bottom of the frame, extending behind the car. the sidewalk is composed of square, evenly spaced tiles.  interactions:  * the car is parked directly adjacent to the sidewalk. * the doors are positioned against the wall.  activities:  * there is no discernible activity taking place in the image.  environment:  * the setting appears to be an exterior urban environment, possibly a street in a historic district. the presence of the vintage car and the style of the doors suggest a potentially older urban area.  notable visual details:  * the lighting is soft and appears to be natural, indicating either early morning or late afternoon. * the texture of the wall is visible, showing the rough surface of the stone or concrete blocks. * the car's design is a classic example of the volkswagen beetle, a popular vintage automobile.\n"
     ]
    }
   ],
   "source": [
    "generate_caption(sys3,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is a dense caption describing the image:  overall description:  the image depicts a man in a dark blue shirt, illuminated by a black background. he is in a video recording.  objects:  *  man:  a young man with fair skin, short brown hair, and a visible beard is the primary subject. he is wearing a dark blue button-down shirt with a dark blue and grey pattern. his mouth is open and he appears to be expressing emotion. he is holding his hands up with his palms facing upward. *  mathematical expression:  a white mathematical expression is displayed on the screen. it is the integral symbol ∫ followed by the function x-1 dx.  interactions:  * the man is visually interacting with the mathematical expression by raising his hands towards it.  activity:  * the image captures a moment from a video recording, likely related to mathematics or educational content.  environment:  * the background is black, suggesting a studio or recording environment.  notable visual details:  * the man's facial expression (grimacing) is prominent. * the white mathematical expression stands out against the black background.\n"
     ]
    }
   ],
   "source": [
    "generate_caption(sys3,img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91720b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is a dense caption describing the visible elements in the image:  overall description:  the image displays a diagram illustrating the concept of forward propagation in a neural network. it shows a layered network with multiple nodes and connections, alongside mathematical expressions representing calculations within the network.  specific elements:  *  neural network diagram:  the image depicts a multi-layered neural network. it consists of three layers, labeled \"layer #1,\" \"layer #2,\" and \"layer #3.\" *  nodes:  each layer contains several nodes, represented by circles, labeled with numerical values like “7-2”, “8-1”, “72”, “92”, “69”, “81”, “75”, and “0.” *  connections:  arrows represent connections between nodes, indicating the flow of data. the connections vary in thickness, and some are indicated with equations. *  equations:  several mathematical equations are present.  * “6+2=8” is written next to the node labeled \"7-2\".  * “2x/5=1” is present next to the node labeled “8-1”.  * “3” is written next to the node labeled “75”.  * “0=y” is written next to the node labeled “0”.  * “x11+x12+x13 = b11” is written below the network diagram. *  text:  the following text is visible: “forward propagation”, “03 march 2022”, “06:07”, “# of trainable parameters” *  color coding:  the diagram employs color coding; primarily red, black, and green.  interactions and activity:  the diagram represents the flow of data through the network during the forward propagation process. the arrows show the transfer of values from one node to the next, accompanied by mathematical computations.\n"
     ]
    }
   ],
   "source": [
    "generate_caption(sys3,img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42fc8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-rag (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
