{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a591dc",
   "metadata": {},
   "source": [
    "# BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3047b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlipProcessor, BlipForConditionalGeneration\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "# Load your image\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# Generate Caption\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "print(\"BLIP Caption:\", processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f1ffb5",
   "metadata": {},
   "source": [
    "# QWEN2.5VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676607ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install qwen-vl-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b755dc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Qwen2_5_VLForConditionalGeneration, AutoProcessor\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqwen_vl_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m process_vision_info\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Qwen2_5_VLForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-VL-7B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-VL-7B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\lib\\site-packages\\qwen_vl_utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision_process\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     extract_vision_info,\n\u001b[0;32m      3\u001b[0m     fetch_image,\n\u001b[0;32m      4\u001b[0m     fetch_video,\n\u001b[0;32m      5\u001b[0m     process_vision_info,\n\u001b[0;32m      6\u001b[0m     smart_resize,\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\lib\\site-packages\\qwen_vl_utils\\vision_process.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": url},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image in great detail.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, _ = process_vision_info(messages)\n",
    "inputs = processor(text=[text], images=image_inputs, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(\"Qwen2.5-VL:\", output_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8933d2",
   "metadata": {},
   "source": [
    "# FLORENCE-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd7b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"../image.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9820de6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor, AutoModelForCausalLM \n\u001b[1;32m----> 8\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m torch_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Florence-2-large\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM \n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)\n",
    "\n",
    "prompt = \"<OD>\"\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    max_new_tokens=4096,\n",
    "    num_beams=3,\n",
    "    do_sample=False\n",
    ")\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n",
    "\n",
    "print(parsed_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd04bfa",
   "metadata": {},
   "source": [
    "# MOLMO 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04751d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5efaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "\n",
    "processor = AutoProcessor.from_pretrained('allenai/Molmo-7B-D-0924', trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained('allenai/Molmo-7B-D-0924', trust_remote_code=True, device_map=\"auto\")\n",
    "\n",
    "inputs = processor.process(images=[image], text=\"Describe this image.\")\n",
    "inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "output = model.generate_from_batch(\n",
    "    inputs, \n",
    "    GenerationConfig(max_new_tokens=200, stop_strings=\"<|endoftext|>\"),\n",
    "    tokenizer=processor.tokenizer\n",
    ")\n",
    "\n",
    "generated_text = processor.tokenizer.decode(output[0, inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "print(\"Molmo:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0676c",
   "metadata": {},
   "source": [
    "# QWEN2.5VL 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98406d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owais\\Downloads\\video-rag\\.venv\\Scripts\\python.exe: No module named uv\n"
     ]
    }
   ],
   "source": [
    "uv pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34a8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owais\\Downloads\\video-rag\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Owais\\Downloads\\video-rag\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Owais\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-VL-3B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 4-bit quantization is the \"secret sauce\" for your 4GB card\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcc158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = processor(\n",
    "    messages,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "\n",
    "output_text = processor.batch_decode(\n",
    "    output_ids, skip_special_tokens=True\n",
    ")[0]\n",
    "\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a56a25",
   "metadata": {},
   "source": [
    "## Gemma 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b6ebf",
   "metadata": {},
   "source": [
    "zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4eb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down what's depicted in this image. It appears to be a visual representation of forward propagation in a neural network, specifically focusing on a simplified two-layer network.\n",
      "\n",
      "**Overall Structure:**\n",
      "\n",
      "*   **Two Layers:**  The diagram shows two layers of neurons. Layer #1 and Layer #2.\n",
      "\n",
      "**Detailed Breakdown:**\n",
      "\n",
      "1.  **Inputs:**\n",
      "    *   `x11`, `x12`, `x13`  - These are the inputs to the first layer.\n",
      "\n",
      "2.  **Layer #1:**\n",
      "    *   **Neurons:** The diagram shows three neurons: `x11`, `x12`, and `x13`.\n",
      "    *   **Weights:**\n",
      "        *   `w11`, `w12`, `w13` - These are the weights connecting the inputs to the first layer's neurons.\n",
      "    *   **Calculations:**\n",
      "        *   `12+2=3` - This shows the first layer's neuron '11' calculating a value using the inputs `x12` and `x11` with a sum.\n",
      "        *   `6+2=8` -  This shows the first layer's neuron '12' calculating a value.\n",
      "        *   `3` - This shows the first layer's neuron '13' calculating a value.\n",
      "\n",
      "3.  **Layer #2:**\n",
      "    *   **Neurons:**  The diagram shows three neurons: `x11`, `x12`, and `x13`.\n",
      "    *   **Weights:**\n",
      "        *   `w21`, `w22`, `w23` - These are the weights connecting the first layer's neurons to the second layer's neurons.\n",
      "    *   **Calculations:**\n",
      "        *   `2x11: 1` - Neuron '11' in the second layer\n",
      "        *   `+1` -  The summation operation.\n",
      "        *   `3` - Neuron '13' in the second layer\n",
      "\n",
      "4.  **Output:**\n",
      "    *   `prediction -> σ(wTx+b)` - The final output of the network is the result of the sigmoid function applied to the weighted sum of the inputs plus a bias.\n",
      "\n",
      "5.  **Biases:**\n",
      "     * `b11`,`b12`, `b13`- These are the biases for each neuron in layer 2.\n",
      "\n",
      "**Key Concepts Illustrated:**\n",
      "\n",
      "*   **Forward Propagation:** The diagram visually demonstrates the process of feeding inputs through the network, performing weighted sums, and applying an activation function.\n",
      "*   **Weights and Biases:** It highlights the key trainable parameters (weights and biases) that are adjusted during the training process.\n",
      "\n",
      "**In essence, the image is illustrating a basic neural network's forward pass calculation.**\n",
      "\n",
      "If you have any specific aspects you'd like me to elaborate on, just let me know!\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "# from pathlib import Path\n",
    "\n",
    "# Pass in the path to the image\n",
    "path ='C:\\\\Users\\\\fareh\\\\coding\\\\video-rag\\\\sample-2.png'\n",
    "\n",
    "\n",
    "response = chat(\n",
    "  model='gemma3',\n",
    "  messages=[\n",
    "      {\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are a helpful assistant that can analyze images and provide captions.\"\n",
    "},\n",
    "\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'What is in this image? .',\n",
    "      'images': [path],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba352e",
   "metadata": {},
   "source": [
    "chain of thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "633f3f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of the image:\n",
      "\n",
      "**Objects:**\n",
      "\n",
      "The image is a diagram illustrating forward propagation in a neural network. Specifically, it shows a simple two-layer neural network. \n",
      "\n",
      "*   **Layers:** There are two layers labeled \"Layer #1\" and \"Layer #2”.\n",
      "*   **Neurons:** Each layer contains neurons (represented by circles) connected by arrows. \n",
      "*   **Weights:** The connections between neurons have associated weights (labeled as w\\_i), such as w\\_11, w\\_12, etc. \n",
      "*   **Activation Functions:** There is an expression for the activation function, σ, denoted as “σ(wx + b)”, where “x” represents the input and “b” is the bias.\n",
      "*   **Output:** There is an output, labeled as “y”.\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "The diagram represents a basic forward pass through a neural network. The arrows show the flow of data from one layer to the next. Each neuron performs a calculation (weighted sum of inputs plus bias) and then applies an activation function. The output of the network is denoted as \"y\".\n",
      "\n",
      "**Concise Answer:**\n",
      "\n",
      "A diagram illustrating forward propagation in a two-layer neural network.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "# from pathlib import Path\n",
    "\n",
    "# Pass in the path to the image\n",
    "path ='C:\\\\Users\\\\fareh\\\\coding\\\\video-rag\\\\sample-2.png'\n",
    "\n",
    "\n",
    "response = chat(\n",
    "  model='gemma3',\n",
    "  messages=[\n",
    "      {\n",
    "          'role':'system',\n",
    "          'content':'''\n",
    "You are a visual reasoning assistant.\n",
    "When given an image, first describe the important objects,\n",
    "then reason step by step, and finally give a concise answer.\n",
    "Do not hallucinate details that are not visible.\n",
    "few examples:\n",
    "Visible Blur: The foreground and parts of the image are out of focus or blurred, indicating either camera movement or subject motion during the shot.\n",
    "Tall, Ornate Buildings: The structures have multiple floors, detailed balconies, and decorative facades, suggesting older or classic urban architecture.\n",
    "Street-Level View: Parked cars line both sides of a road or narrow street, confirming an urban environment with typical city traffic and infrastructure.\n",
    "Soft, Warm Light: The sunlight appears to be hitting the buildings at an angle, creating a warm glow on the façade and enhancing the sense of a real city scene rather than a staged setup.\n",
    "\n",
    "Final Caption: A blurry photo of a city street with buildings. '''\n",
    "      },\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'What is in this image? .',\n",
    "      'images': [path],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507548a4",
   "metadata": {},
   "source": [
    "object detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d8f6720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary='The image shows a screenshot of a neural network diagram, likely created in a software like MATLAB or similar. It depicts a two-layer feedforward neural network with connections and associated numerical values. The diagram includes matrices representing weights and biases, along with a sigmoid activation function.' objects=[Object(name='Neural Network Diagram', confidence=0.98, attributes='Two-layer feedforward, showing connections between nodes and matrices representing weights and biases.'), Object(name='Matrix', confidence=0.95, attributes='Represents weights (w) and biases (b) for the neural network. Includes values like w11, w12, w13, w21, w22, w23, w31, w32, w33, b11, b12, b13, b21, b22, b23, b31, b32, b33.'), Object(name='Sigmoid Activation Function', confidence=0.9, attributes=\"Represented by the equation 'σ(w^Tx + b)'\"), Object(name='Numbers', confidence=0.99, attributes='Various numerical values are displayed within the diagram, including 1, 2, 3, 5, 7, 8, 9, 15, 12, 15, 72, 69, 81, 92, 75, 0, 26, 1, 3, 1, 0.'), Object(name='Text', confidence=0.97, attributes=\"Includes labels like 'Layer #1', 'Forward Propagation', '# of trainable parameters', and 'prediction'.\")] scene='Computer Screen' colors=['black', 'white', 'green', 'red', 'blue'] time_of_day='Morning' setting='Unknown' text_content=None\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal, Optional\n",
    "\n",
    "path ='C:\\\\Users\\\\fareh\\\\coding\\\\video-rag\\\\sample-2.png'\n",
    "class Object(BaseModel):\n",
    "  name: str\n",
    "  confidence: float\n",
    "  attributes: str\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "  summary: str\n",
    "  objects: list[Object]\n",
    "  scene: str\n",
    "  colors: list[str]\n",
    "  time_of_day: Literal['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "  setting: Literal['Indoor', 'Outdoor', 'Unknown']\n",
    "  text_content: Optional[str] = None\n",
    "\n",
    "response = chat(\n",
    "  model='gemma3',\n",
    "  messages=[{\n",
    "    'role': 'user',\n",
    "    'content': 'Describe this photo and list the objects you detect.',\n",
    "    'images': [path]\n",
    "  }],\n",
    "  format=ImageDescription.model_json_schema(),\n",
    "  options={'temperature': 0},\n",
    ")\n",
    "\n",
    "image_description = ImageDescription.model_validate_json(response.message.content)\n",
    "print(image_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f582b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-rag (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
