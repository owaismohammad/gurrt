{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a18880",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Explain this concept in simple terms.\",\n",
    "    \n",
    "    \"So basically I was reading something about machines learning patterns and stuff and retrieving documents and then answering questions from them, what is that method called?\",\n",
    "    \n",
    "    \"What is retreival augmnted genration and how dose it work?\",\n",
    "    \n",
    "    \"vector\",\n",
    "    \n",
    "    \"I am not sure if this is the right term but I think I heard in a lecture that there is a way where instead of training a model again and again you keep knowledge somewhere else and the model looks it up when answering questions, I forgot the name but can you explain how that works and why people use it?\",\n",
    "    \n",
    "    \"Explain quicksort, blockchain hashing, and how embeddings are used in semantic search.\",\n",
    "    \n",
    "    \"Why is RAG worse than fine-tuning and also better than fine-tuning at the same time?\",\n",
    "    \n",
    "    \"Explain the second method mentioned earlier in detail.\",\n",
    "    \n",
    "    \"How do AI systems look up information from stored knowledge before answering a question?\",\n",
    "    \n",
    "    \"What problem does this architecture solve?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d78ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries2 = [\n",
    "    \"What is retrieval augmented generation?\",\n",
    "    \n",
    "    \"Explain how RAG works in simple terms.\",\n",
    "    \n",
    "    \"What are the main components of a RAG pipeline and what does each component do?\",\n",
    "    \n",
    "    \"Compare RAG with fine-tuning in terms of cost, performance, and real-world use cases.\",\n",
    "    \n",
    "    \"How can I build a RAG system using Python, embeddings, and a vector database? Give high level steps.\",\n",
    "    \n",
    "    \"I am building a chatbot that answers questions from PDFs. What is the best way to chunk documents and store embeddings for accurate retrieval?\",\n",
    "    \n",
    "    \"What problems occur in retrieval augmented generation systems such as hallucination, poor retrieval, and context window limits, and how can they be solved?\",\n",
    "    \n",
    "    \"Suppose I store videos and transcripts in a cloud storage service and want to build a video RAG system that answers questions about specific scenes. What architecture would you recommend?\",\n",
    "    \n",
    "    \"Give an example workflow showing how a user query is processed step by step in a production-grade RAG system including embedding generation, retrieval, reranking, and response generation.\",\n",
    "    \n",
    "    \"Design a scalable RAG architecture for a startup that expects millions of queries per day and needs low latency, high accuracy, and cost optimization. Include infrastructure considerations.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "228db2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries3=[\n",
    "  \"What is happening in this video?\",\n",
    "  \"Who are the main people appearing in the video?\",\n",
    "  \"What is the funniest moment in this video?\",\n",
    "  \"What activity are they doing at the beginning?\",\n",
    "  \"Where was this video filmed?\",\n",
    "  \"What happens near the end of the video?\",\n",
    "  \"Why were they laughing in the middle part?\",\n",
    "  \"What is the main theme or story of the video?\",\n",
    "  \"Is there any interesting or surprising moment?\",\n",
    "  \"What message or takeaway does the video give?\",\n",
    "  \"What was that joke they made about the food?\",\n",
    "  \"What were they doing when the music started?\",\n",
    "  \"Who was the person wearing the red shirt?\",\n",
    "  \"Why did everyone start cheering at that point?\",\n",
    "  \"What happened right after they reached that place?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ec3c4",
   "metadata": {},
   "source": [
    "## simple query rewritting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa65227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "def querry_rewrite(query):\n",
    "    response = chat(\n",
    "        model=\"gemma3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": '''You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "    Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.give rewritten quey only without any explanation.'''},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb91c7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you please provide me with a link to the video or describe the video's content, including the topic, speakers, and any key visuals?\n",
      "\n",
      "---\n",
      "\n",
      "Who are the key individuals featured in the video, and what are their roles or affiliations?\n",
      "\n",
      "---\n",
      "\n",
      "What are the specific moments in this video that you found most humorous, and could you describe those moments in detail?\n",
      "\n",
      "---\n",
      "\n",
      "What specific activity is the character or subject engaged in at the very start of the provided text or document?\n",
      "\n",
      "---\n",
      "\n",
      "What is the exact location where the footage in this video was filmed, including specific addresses, landmarks, or geographic coordinates if available?\n",
      "\n",
      "---\n",
      "\n",
      "What are the specific events or details that occur in the final 30 seconds of the video?\n",
      "\n",
      "---\n",
      "\n",
      "What specific scene in the video or text are you referring to when people were laughing in the middle part, and what context or details might explain the reason for their laughter?\n",
      "\n",
      "---\n",
      "\n",
      "What are the key narrative elements, central arguments, and overall message conveyed in the video, and how do these elements contribute to the video’s core theme?\n",
      "\n",
      "---\n",
      "\n",
      "Can you tell me about a specific historical event or scientific discovery that had an unexpected or surprising outcome?\n",
      "\n",
      "---\n",
      "\n",
      "What is the central argument or key message conveyed in the video, and what specific evidence or examples does the video use to support this message?\n",
      "\n",
      "---\n",
      "\n",
      "What was the specific joke involving food that was recently discussed or shared, and could you provide any context about the topic or speaker if possible?\n",
      "\n",
      "---\n",
      "\n",
      "What specific actions were individuals engaged in immediately prior to the commencement of the musical piece, including their postures, interactions, and apparent focus of attention?\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "Can you please provide more context regarding the image or situation you are referring to? Specifically, I need details such as the location, time, and any identifying features of the individuals present to accurately identify the person wearing the red shirt.\n",
      "\n",
      "---\n",
      "\n",
      "What specific event or moment led to a sudden increase in cheering, and what was the context surrounding that event (e.g., the sport, the location, the people involved)?\n",
      "\n",
      "---\n",
      "\n",
      "What specific events occurred immediately following the arrival at [location name] on [date] as described in [source material]?\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in queries3:\n",
    "    querry_rewrite(query)\n",
    "    print(\"\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed89161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the technical details of retrieval-augmented generation, including the roles of the retrieval component (e.g., vector databases, search algorithms) and the generation component (e.g., large language models), and how they interact to produce an output.  Specifically, describe the process of querying a knowledge base, selecting relevant context, and using that context to inform the generation of a response.  Also, discuss common challenges and optimization strategies within this architecture.\n",
      "\n",
      "---\n",
      "\n",
      "Describe the architecture and data flow of a Retrieval-Augmented Generation (RAG) system, including the roles of the knowledge base, retriever, and generator, and how they interact to produce a response. Specifically, detail the process of embedding documents, creating a vector database, querying the database with a user prompt, and using a large language model (LLM) to generate a final answer based on the retrieved context. Explain the benefits of RAG over simply using an LLM with a prompt, highlighting the advantages of accessing and incorporating external knowledge.\n",
      "\n",
      "---\n",
      "\n",
      "Describe the architecture of a Retrieval-Augmented Generation (RAG) pipeline, detailing the roles of each stage, including document retrieval, chunking strategies, embedding generation, vector storage, and the Generative AI model's interaction with retrieved context. Provide specific examples of techniques used within each stage and explain how these components work together to enable effective information retrieval and generation.\n",
      "\n",
      "---\n",
      "\n",
      "Evaluate the cost-effectiveness, retrieval accuracy, and practical applications of Retrieval-Augmented Generation (RAG) versus fine-tuning large language models for a specific task, considering factors like dataset size, model complexity, and desired output quality.\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "What are the high-level steps involved in creating a RAG (Retrieval-Augmented Generation) system using Python, embeddings, and a vector database, specifically focusing on the data ingestion, embedding generation, vector storage, and retrieval processes? Detail the necessary steps for setting up a system capable of retrieving relevant context for a language model based on user queries, including considerations for choosing appropriate embedding models and vector database technologies.\n",
      "\n",
      "---\n",
      "\n",
      "What are the optimal chunking strategies and embedding models for creating a RAG system that accurately retrieves answers from PDF documents, considering factors like document length, semantic similarity, and the types of questions the chatbot will need to answer?\n",
      "\n",
      "---\n",
      "\n",
      "What are the specific challenges of Retrieval-Augmented Generation (RAG) systems, including the technical causes of hallucination, the root causes of ineffective retrieval, and the practical limitations imposed by context window size constraints, and what are the most effective strategies for mitigating each of these issues through techniques like fine-tuning, advanced retrieval methods, and optimized context management?\n",
      "\n",
      "---\n",
      "\n",
      "Design a video RAG system using a modular architecture incorporating a vector database, a video processing pipeline, and a retrieval layer optimized for scene-specific question answering, including pre-processing steps for both video and transcript data, semantic search leveraging embeddings, and a response generation component that combines retrieved video segments and transcript snippets for contextually accurate answers.\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "1.  **User Query Input:** The user submits a query (e.g., \"What are the best strategies for improving website SEO?\").\n",
      "2.  **Query Understanding & Chunking:** The query is processed to identify key entities and intent. The query is then broken down into smaller, manageable chunks (e.g., sentences or paragraphs) suitable for embedding generation.\n",
      "3.  **Embedding Generation:** Each chunk is converted into a numerical vector representation (embedding) using a pre-trained embedding model (e.g., OpenAI's `text-embedding-ada-002`).\n",
      "4.  **Retrieval:** The user's query embedding is also generated. A vector database (e.g., Pinecone, Weaviate, Chroma) is searched for chunks with embeddings most similar to the query embedding, based on cosine similarity or another distance metric.  A top-k number of chunks are retrieved.\n",
      "5.  **Reranking:** The retrieved chunks are reranked based on multiple factors, including:\n",
      "    *   Similarity score from the initial retrieval.\n",
      "    *   Relevance to the query's entities.\n",
      "    *   Chunk length (favoring longer, more informative chunks).\n",
      "    *   Metadata associated with the chunks (e.g., source document, date).  A learned or rule-based reranker might be employed.\n",
      "6.  **Context Construction:** The highest-ranked chunks are combined to create a context string.\n",
      "7.  **Response Generation:** A large language model (LLM) (e.g., GPT-3.5, PaLM) receives the context string and the original user query as input. It generates a comprehensive and relevant response, utilizing the retrieved information.  Prompt engineering is key here to instruct the LLM on how to use the context.\n",
      "8.  **Response Formatting & Delivery:** The LLM's response is formatted (e.g., markdown, bullet points) and presented to the user. \n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "Design a hybrid RAG architecture leveraging a vector database, a knowledge graph, and a retrieval layer optimized for high throughput, low latency, and cost-effectiveness, utilizing a tiered caching strategy and serverless components for scalability. Implement a robust A/B testing framework for continuous performance optimization, alongside automated monitoring and anomaly detection for proactive issue resolution. Focus on incremental indexing and frequent knowledge base updates to maintain accuracy while minimizing operational overhead. Prioritize a multi-cloud deployment strategy with geographically distributed nodes to ensure resilience and reduce latency for global users. Utilize cost-effective GPU instances for vector embeddings and explore quantization techniques to reduce model size and inference costs. Employ a dynamic query routing system based on query complexity and data relevance to intelligently direct queries to the most appropriate retrieval path.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in queries2:\n",
    "    querry_rewrite(query)\n",
    "    print(\"\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40442c3d",
   "metadata": {},
   "source": [
    "## subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20992fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_decompose = \"\"\"You are a helpful assistant  Given a user question, break it down into distinct 2-3 sub questions only no answer .\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed3aa9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "def querry_rewrite_subquery(query):\n",
    "    response = chat(\n",
    "        model=\"gemma3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_decompose},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b352e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A group of archaeologists are carefully excavating a previously undiscovered Roman bath complex beneath the ruins of Pompeii. They’ve just unearthed a remarkably well-preserved hypocaust system, including intricate heating channels and stunning mosaics depicting scenes from mythology. Initial analysis suggests the baths were significantly larger and more elaborate than previously thought, indicating a higher status residential area existed there.\n",
      "\n",
      "---\n",
      "\n",
      "Mark Zuckerberg, Sheryl Sandberg, and Andy Jassy.\n",
      "\n",
      "---\n",
      "\n",
      "The unexpected sheep jumping out during the chase scene.\n",
      "\n",
      "---\n",
      "\n",
      "They're meticulously arranging seashells on a driftwood log.\n",
      "\n",
      "---\n",
      "\n",
      "It appears to be shot on location in the Scottish Highlands, specifically near Loch Ness.\n",
      "\n",
      "---\n",
      "\n",
      "They discover a hidden portal leading to a parallel dimension.\n",
      "\n",
      "---\n",
      "\n",
      "It was a recording of a synchronized swimming routine gone horribly wrong – the diver’s timing was off, and she landed directly on top of the inflatable dolphin.\n",
      "\n",
      "---\n",
      "\n",
      "The video explores the cyclical nature of grief and how unresolved trauma can manifest in unexpected ways across generations.\n",
      "\n",
      "---\n",
      "\n",
      "The hummingbird froze, suspended mid-air, its wings a blur of impossible speed, before gently landing on my outstretched finger and… humming.\n",
      "\n",
      "---\n",
      "\n",
      "Never underestimate the power of a single act of kindness.\n",
      "\n",
      "---\n",
      "\n",
      "Why did the tomato blush? Because it saw the salad dressing!\n",
      "\n",
      "---\n",
      "\n",
      "They were arguing quietly about the thermostat.\n",
      "\n",
      "---\n",
      "\n",
      "It was Elias Thorne.\n",
      "\n",
      "---\n",
      "\n",
      "The crowd erupted when the goalie made a miraculous save, deflecting the puck with his glove right before it crossed the goal line.\n",
      "\n",
      "---\n",
      "\n",
      "The air shimmered, and a chorus of voices, not quite human, filled the chamber.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in queries3:\n",
    "    querry_rewrite_subquery(query)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3bfa3f",
   "metadata": {},
   "source": [
    "## hyde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec2bf7",
   "metadata": {},
   "source": [
    "hyde nhi lag skta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9c226ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_hyde = \"\"\"You are a helpful assistant that generates hypothetical answers to user queries. Given a user query, generate a plausible hypothetical answer that would be relevant to the query. Do not provide any explanation or context beyond the hypothetical answer itself.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0de7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "def querry_rewrite_hyde(query):\n",
    "    response = chat(\n",
    "        model=\"gemma3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_hyde},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0e709ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A group of scientists are observing a newly discovered species of bioluminescent jellyfish in a deep-sea submersible. They are documenting its unusual behavior and stunning light displays.\n",
      "\n",
      "---\n",
      "\n",
      "Mark Zuckerberg, Sheryl Sandberg, and Andy Jassy.\n",
      "\n",
      "---\n",
      "\n",
      "The squirrel stealing the hotdog is definitely the funniest moment.\n",
      "\n",
      "---\n",
      "\n",
      "They’re meticulously arranging wildflowers in a vintage porcelain vase.\n",
      "\n",
      "---\n",
      "\n",
      "It appears to be a remote area of the Patagonian Andes in Argentina.\n",
      "\n",
      "---\n",
      "\n",
      "They discover a hidden portal leading to an alternate dimension where time flows backward.\n",
      "\n",
      "---\n",
      "\n",
      "It was a recording of a misidentified bird call – a rare type of sandpiper that sounds remarkably like human laughter when it flies.\n",
      "\n",
      "---\n",
      "\n",
      "The video explores the cyclical nature of grief and how unresolved trauma can manifest across generations, ultimately highlighting the importance of acknowledging and confronting the past to break free from destructive patterns.\n",
      "\n",
      "---\n",
      "\n",
      "During the excavation, the team unearthed a perfectly preserved Roman mosaic depicting a scene of a unicorn battling a griffin – a detail completely absent from all previous historical records of the site.\n",
      "\n",
      "---\n",
      "\n",
      "Embrace the unexpected, and you might discover your greatest passions.\n",
      "\n",
      "---\n",
      "\n",
      "Why did the tomato blush? Because it saw the salad dressing!\n",
      "\n",
      "---\n",
      "\n",
      "They were arguing quietly, but intensely, about the inheritance.\n",
      "\n",
      "---\n",
      "\n",
      "It was Elias Vance.\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries3:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mquerry_rewrite_hyde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mquerry_rewrite_hyde\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquerry_rewrite_hyde\u001b[39m(query):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     response = \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemma3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_hyde\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(response.message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\ollama\\_client.py:365\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, logprobs, top_logprobs, format, options, keep_alive)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    319\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    320\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    331\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    332\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    334\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m      \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\ollama\\_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\ollama\\_client.py:129\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    128\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     r.raise_for_status()\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for query in queries3:\n",
    "    querry_rewrite_hyde(query)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8e1855",
   "metadata": {},
   "source": [
    "## step back prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4608f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_step_back = \"\"\"You are an expert at taking a specific question and extracting a more generic question that gets at the underlying principles needed to answer the specific question.\n",
    "\n",
    "Given a specific user question, write a more generic question that needs to be answered in order to answer the specific question.\n",
    "\n",
    "If you don't recognize a word or acronym do not try to rewrite it.\n",
    "\n",
    "Write concise questions.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c39a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "def querry_rewrite_step_back(query):\n",
    "    response = chat(\n",
    "        model=\"gemma3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_step_back},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc65cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the core principles behind this concept?\n",
      "\n",
      "---\n",
      "\n",
      "What are the underlying techniques used for information retrieval and question answering?\n",
      "\n",
      "---\n",
      "\n",
      "What are the core principles of combining large language models with external knowledge?\n",
      "\n",
      "---\n",
      "\n",
      "What is a vector?\n",
      "\n",
      "---\n",
      "\n",
      "How can knowledge be effectively stored and accessed by a model for answering questions?\n",
      "\n",
      "---\n",
      "\n",
      "Okay, here are the generic questions derived from those specific topics:\n",
      "\n",
      "1.  **Quicksort:** What are the core principles of efficient sorting algorithms?\n",
      "2.  **Blockchain Hashing:** What are the properties of cryptographic hash functions and how are they used in secure systems?\n",
      "3.  **Embeddings & Semantic Search:** How can vector representations of data capture semantic meaning for search applications?\n",
      "\n",
      "---\n",
      "\n",
      "What are the comparative strengths and weaknesses of Retrieval-Augmented Generation (RAG) and fine-tuning for language models?\n",
      "\n",
      "---\n",
      "\n",
      "What are the key principles behind the method being discussed?\n",
      "\n",
      "---\n",
      "\n",
      "What are the core mechanisms for knowledge retrieval in AI systems?\n",
      "\n",
      "---\n",
      "\n",
      "What is the core challenge this design addresses?\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    querry_rewrite_step_back(query)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ffa2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the key events and their significance?\n",
      "\n",
      "---\n",
      "\n",
      "Who are the key figures in this context?\n",
      "\n",
      "---\n",
      "\n",
      "What makes something humorous?\n",
      "\n",
      "---\n",
      "\n",
      "What is the context of the situation?\n",
      "\n",
      "---\n",
      "\n",
      "Where was the location depicted in this video?\n",
      "\n",
      "---\n",
      "\n",
      "What is the overall narrative arc of this story?\n",
      "\n",
      "---\n",
      "\n",
      "What caused the observed behavior?\n",
      "\n",
      "---\n",
      "\n",
      "What are the core ideas presented?\n",
      "\n",
      "---\n",
      "\n",
      "What factors contribute to a memorable moment?\n",
      "\n",
      "---\n",
      "\n",
      "What is the core idea of the content?\n",
      "\n",
      "---\n",
      "\n",
      "What is considered funny?\n",
      "\n",
      "---\n",
      "\n",
      "What were people doing before a specific event began?\n",
      "\n",
      "---\n",
      "\n",
      "Who was the person in the image?\n",
      "\n",
      "---\n",
      "\n",
      "What factors influence crowd behavior and enthusiasm?\n",
      "\n",
      "---\n",
      "\n",
      "What occurred following that event?\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in queries3:\n",
    "    querry_rewrite_step_back(query)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e79ee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Video downloaded successfully and saved to ./videos/downloaded_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "def download_video(url,save_path):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 8192\n",
    "        downloaded = 0\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=block_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        percent = (downloaded / total_size) * 100\n",
    "                        print(f\"\\rDownloading: {percent:.2f}%\", end='', flush=True)\n",
    "        print(f\"\\nVideo downloaded successfully and saved to {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading video: {e}\")\n",
    "\n",
    "# Example usage\n",
    "video_url = \"https://youtu.be/SQUtermA2rA?si=sbAgnbeGeRSfDyrZ\"\n",
    "save_path = \"./videos/downloaded_video.mp4\"\n",
    "download_video(video_url,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdbfc547",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "os.path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabc8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-rag (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
