{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64f138d",
   "metadata": {},
   "source": [
    "### Shot-Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344512bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_scenes(video_path, scene_list):\n",
    "#     cap = cv2.VideoCapture(video_path)   \n",
    "#     embeddings = []\n",
    "#     metadatas = []\n",
    "#     ids = []\n",
    "#     with tqdm(total = len(scene_list), desc = \"Processing frames\") as pbar:\n",
    "#         for i, scene in enumerate(scene_list):\n",
    "#             start_time, end_time = scene[0].get_seconds(), scene[1].get_seconds()\n",
    "#             mid_time = (start_time + end_time) / 2\n",
    "#             timestamps = [start_time, mid_time, end_time]\n",
    "#             labels = [\"initial\", \"middle\", \"final\"]\n",
    "            \n",
    "#             for t, label in zip(timestamps, labels):\n",
    "#                 cap.set(cv2.CAP_PROP_POS_MSEC, t * 1000)\n",
    "#                 ret, frame = cap.read()\n",
    "                \n",
    "#                 if ret:\n",
    "#                     img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#                     image = Image.fromarray(img_rgb)\n",
    "#                     inputs = processor(images = image, return_tensors = 'pt').to(device)\n",
    "#                     blip_input = blip_processor(images = image, return_tensors = 'pt').to(device)\n",
    "#                     with torch.no_grad():\n",
    "#                         outputs = model.get_image_features(inputs.pixel_values)\n",
    "#                         blip_outputs = blip_model.generate(**blip_input,\n",
    "#                                                         # max_length = 500,\n",
    "#                                                         # min_length = 150,\n",
    "#                                                         # no_repeat_ngram_size=2,\n",
    "#                                                         # num_beams = 5,\n",
    "#                                                         )\n",
    "                    \n",
    "#                     caption = blip_processor.decode(blip_outputs[0], skip_special_tokens=True)\n",
    "#                     image_embedding = outputs.pooler_output\n",
    "#                     image_embedding = image_embedding / image_embedding.norm(dim = -1, keepdim= True)\n",
    "#                     image_embedding = image_embedding.squeeze(0).cpu().numpy().tolist()\n",
    "#                     timestamp_sec = t*1000\n",
    "#                     frame_id = f\"{video_path}:{timestamp_sec}\"\n",
    "                \n",
    "#                     ids.append(frame_id)\n",
    "#                     embeddings.append(image_embedding)\n",
    "\n",
    "#                     metadatas.append({\n",
    "#                         \"frame_idx\": f\"frame_no_{i}_{label}\",\n",
    "#                         \"caption\": caption,\n",
    "#                         \"timestamp_ms\": timestamp_sec,\n",
    "#                         \"source_path\": video_path\n",
    "#                     })\n",
    "#             pbar.update(1)\n",
    "                \n",
    "#     return embeddings, metadatas, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66380e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_scenes(video_path, scene_list):\n",
    "#     cap = cv2.VideoCapture(video_path)   \n",
    "#     embeddings = []\n",
    "#     metadatas = []\n",
    "#     ids = []\n",
    "#     with tqdm(total = len(scene_list), desc = \"Processing frames\") as pbar:\n",
    "#         for i, scene in enumerate(scene_list):\n",
    "#             start_time, end_time = scene[0].get_seconds(), scene[1].get_seconds()\n",
    "#             mid_time = (start_time + end_time) / 2\n",
    "#             timestamps = mid_time\n",
    "#             labels = \"middle\"\n",
    "            \n",
    "            \n",
    "#             cap.set(cv2.CAP_PROP_POS_MSEC, timestamps * 1000)\n",
    "#             ret, frame = cap.read()\n",
    "            \n",
    "#             if ret:\n",
    "#                 img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#                 image = Image.fromarray(img_rgb)\n",
    "#                 inputs = processor(images = image, return_tensors = 'pt').to(device)\n",
    "#                 blip_input = blip_processor(images = image, return_tensors = 'pt').to(device)\n",
    "#                 with torch.no_grad():\n",
    "#                     outputs = model.get_image_features(inputs.pixel_values)\n",
    "#                     blip_outputs = blip_model.generate(**blip_input,\n",
    "#                                                     # max_length = 60,\n",
    "#                                                     # min_length = 20,\n",
    "#                                                     # no_repeat_ngram_size=2,\n",
    "#                                                     # num_beams = 5,\n",
    "#                                                     )\n",
    "                \n",
    "#                 caption = blip_processor.decode(blip_outputs[0], skip_special_tokens=True)\n",
    "#                 image_embedding = outputs.pooler_output\n",
    "#                 image_embedding = image_embedding / image_embedding.norm(dim = -1, keepdim= True)\n",
    "#                 image_embedding = image_embedding.squeeze(0).cpu().numpy().tolist()\n",
    "#                 timestamp_sec = timestamps*1000\n",
    "#                 frame_id = f\"{video_path}:{timestamp_sec}\"\n",
    "            \n",
    "#                 ids.append(frame_id)\n",
    "#                 embeddings.append(image_embedding)\n",
    "\n",
    "#                 metadatas.append({\n",
    "#                     \"frame_idx\": f\"frame_no_{i}_{labels}\",\n",
    "#                     \"caption\": caption,\n",
    "#                     \"timestamp_ms\": timestamp_sec,\n",
    "#                     \"source_path\": video_path\n",
    "#                 })\n",
    "#             pbar.update(1)\n",
    "                \n",
    "#     return embeddings, metadatas, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d6de93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.13.0.92-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=2 in c:\\users\\fareh\\appdata\\roaming\\python\\python314\\site-packages (from opencv-python) (2.4.2)\n",
      "Downloading opencv_python-4.13.0.92-cp37-abi3-win_amd64.whl (40.2 MB)\n",
      "   ---------------------------------------- 0.0/40.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.1/40.2 MB 16.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.7/40.2 MB 14.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 7.6/40.2 MB 13.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 10.2/40.2 MB 13.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.1/40.2 MB 13.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 15.7/40.2 MB 13.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 18.4/40.2 MB 13.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 21.2/40.2 MB 13.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 23.9/40.2 MB 13.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 26.5/40.2 MB 13.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 29.1/40.2 MB 13.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 31.7/40.2 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 34.6/40.2 MB 13.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 37.2/40.2 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.8/40.2 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 40.2/40.2 MB 13.2 MB/s  0:00:03\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.13.0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: C:\\Python314\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417d8e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fareh\\coding\\video-rag\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d031885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 398/398 [00:00<00:00, 1030.61it/s, Materializing param=visual_projection.weight]                                \n",
      "\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "The image processor of type `CLIPImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n",
      "The image processor of type `BlipImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n",
      "Loading weights: 100%|██████████| 616/616 [00:00<00:00, 902.13it/s, Materializing param=vision_model.post_layernorm.weight]                                       \n",
      "The tied weights mapping and config for this model specifies to tie text_decoder.bert.embeddings.word_embeddings.weight to text_decoder.cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "\u001b[1mBlipForConditionalGeneration LOAD REPORT\u001b[0m from: Salesforce/blip-image-captioning-large\n",
      "Key                                       | Status     |  | \n",
      "------------------------------------------+------------+--+-\n",
      "text_decoder.bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from scenedetect import open_video, SceneManager\n",
    "import scenedetect\n",
    "from scenedetect.detectors import ContentDetector\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\",use_safetensors=True).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c148876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scene_split(video_path):\n",
    "    print(\"--- Detecting shot boundaries with PySceneDetect ---\")\n",
    "    video = open_video(video_path)\n",
    "    scene_manager = SceneManager()\n",
    "    scene_manager.add_detector(ContentDetector())\n",
    "\n",
    "    try:\n",
    "        scene_manager.detect_scenes(video, show_progress=True)\n",
    "        scene_list = scene_manager.get_scene_list()\n",
    "    except Exception as e:\n",
    "        print(\"Scene detection failed:\", e)\n",
    "        scene_list = []\n",
    "    return scene_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d677b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions_in_batches(batch_of_frames, clip_model, clip_processor, blip_model, blip_processor, device=\"cuda\"):\n",
    "    clip_inputs = clip_processor(images=batch_of_frames, return_tensors=\"pt\").to(device)\n",
    "    blip_inputs = blip_processor(images = batch_of_frames, return_tensors = 'pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_outputs = clip_model.get_image_features(clip_inputs.pixel_values)\n",
    "        clip_outputs = clip_outputs.pooler_output\n",
    "        clip_embeddings = clip_outputs / clip_outputs.norm(p=2, dim=-1, keepdim=True)\n",
    "        blip_output_ids = blip_model.generate(**blip_inputs,\n",
    "                                        max_length = 300,\n",
    "                                        min_length = 100,\n",
    "                                        no_repeat_ngram_size=3,\n",
    "                                        repetition_penalty=1.5,\n",
    "                                        early_stopping=True,\n",
    "                                        do_sample=False,\n",
    "                                        num_beams = 3,\n",
    "                                        )\n",
    "        captions = blip_processor.batch_decode(blip_output_ids, skip_special_tokens=True)\n",
    "\n",
    "   \n",
    "            \n",
    "    embeddings_list = clip_embeddings.cpu().numpy().tolist()\n",
    "    del clip_inputs, blip_inputs, clip_outputs, blip_output_ids\n",
    "    torch.cuda.empty_cache()        \n",
    "\n",
    "    return captions, embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2e7a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_listing(scene_list, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)   \n",
    "    ids = []\n",
    "    frame_PIL = []\n",
    "    timestamps_list = []\n",
    "    with tqdm(total = len(scene_list), desc=\"processing frames \") as pbar: \n",
    "        for i, scene in enumerate(scene_list):\n",
    "            start_time, end_time = scene[0].get_seconds(), scene[1].get_seconds()\n",
    "            mid_time = (start_time + end_time) / 2\n",
    "            timestamps = [start_time, mid_time, end_time]\n",
    "            labels = [\"initial\", \"middle\", \"final\"]\n",
    "            \n",
    "            for t, label in zip(timestamps, labels):\n",
    "                cap.set(cv2.CAP_PROP_POS_MSEC, t * 1000)\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if ret:\n",
    "                    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    image = Image.fromarray(img_rgb)\n",
    "                    frame_PIL.append(image)\n",
    "                    timestamps_list.append(t)\n",
    "                frame_id = f\"{video_path}:{t}\"\n",
    "                ids.append(frame_id)\n",
    "            pbar.update(1)\n",
    "    return frame_PIL, timestamps_list, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f82f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_captioning(frame_list: list, batch_size: int):\n",
    "    caption_list = []\n",
    "    embedding_list = []\n",
    "    \n",
    "    with tqdm(total = int(len(frame_list)/ batch_size), desc = \"Batched image captioning\") as pbar:\n",
    "        for i in range(0, len(frame_list),batch_size):\n",
    "            batch = frame_list[i:i+batch_size]\n",
    "            caption , embedding = generate_captions_in_batches(batch, \n",
    "                                                            clip_model= model,\n",
    "                                                            clip_processor= processor,\n",
    "                                                            blip_model= blip_model,\n",
    "                                                            blip_processor= blip_processor)\n",
    "            caption_list.extend(caption)\n",
    "            embedding_list.extend(embedding)\n",
    "            pbar.update(1)\n",
    "    return caption_list, embedding_list           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a427d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"../This Integral Breaks Math.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c1fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Detecting shot boundaries with PySceneDetect ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Detected: 48 | Progress: 100%|██████████| 7793/7793 [00:27<00:00, 286.10frames/s]\n"
     ]
    }
   ],
   "source": [
    "scene_list = scene_split(\"../This Integral Breaks Math.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5514bb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing frames :   0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing frames : 100%|██████████| 49/49 [00:07<00:00,  6.27it/s]\n"
     ]
    }
   ],
   "source": [
    "frame_PIL, timestamps_list, ids = frame_listing(scene_list= scene_list, video_path= video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "365cebf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batched image captioning: 19it [01:54,  6.05s/it]                        \n"
     ]
    }
   ],
   "source": [
    "caption_list, embeddings_list = batched_captioning(frame_list= frame_PIL, batch_size=8)  #300,100 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c9065fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146,\n",
       " ['there is a man standing in front of a black background holding up two fingers and making the peace sign with one hand, while the other hand is pointing at the same direction on the other side of the other way with the other finger to the sign that says 3x - 1 - 4 - 1 / 1 - 2 - 5 - 3 - 1 0 - 1s - 1? - 1 % - - - 7 - - 11 - - 9 - - 6 - - 8 - - 10 -',\n",
       "  \"there is a man standing in front of a blackboard with the word 3 - 1 written on it and pointing his fingers at the same time as if he does not know what to do you are doing this trick or not to make the other way to solve the problem for the problem, i ' s 2 - 4 - 5 - 1 - 0 - 3 - 2 - 7 - 1 / 2 - 1? - 3 / 3 - 4 / 3 / 2 / 3x - 2 / 1\",\n",
       "  \"this is an image of a person writing on a blackboard with a calculator in the foreground and a hand holding a pen in front of their left hand, pointing to the sign that says ' s x - x - 1x - i - 1 - 2 - 3 - 4 - 5 - 1 / 3 - - - 0 - - 1s - - o - - 6 - - 9 - - 7 - - 8 - - 10 - - 3c - - 11 - 1\",\n",
       "  \"this is an image of a person writing on a blackboard with a calculator in the foreground and a hand holding a pen in front of their left hand, pointing to the sign that says ' s x - x - 1x - i - 1 - 2 - 3 - 4 - 5 - 1 / 3 - - - 0 - - 1s - - o - - 6 - - 9 - - 7 - - 8 - - 10 - - 3c - - 11 - 1\",\n",
       "  \"there is a picture of a black background with the word ' x ' written in white letters on the left side of the image, and an x - y - x - i - y sign on the right handwritten in the middle of the bottom corner of the picture is a white line across the image of the top of the frame that is a black text, and the picture of the second letter, which is a small, and a yellow arrow, at the same time, on the opposite,\",\n",
       "  \"this is an image of a man standing in front of a blackboard with the word ' s x - 1 written on it and he is holding his hands up to his chest, and looking at the camera while wearing a gray t - shirt that has his right hand over his left side of his left arm and his right shoulder, and his left hand folded out, and the other half of his right, as he is pointing to the other hand to his left, with a small, and one\",\n",
       "  \"this is an image of a man standing in front of a blackboard with the word ' s x - 1 written on it and he is holding his hands up to his chest, and looking at the camera while wearing a gray t - shirt that has his right hand over his left side of his left arm and his right shoulder, and his left hand folded out, and the other half of his right, as he is pointing to the other hand to his left, with a small, and one\",\n",
       "  \"there is a man standing in front of a blackboard with the word sx - 1 written on it and pointing to the left side of the board with both hands, and the other hand up at the same direction of the sign that indicates the same point as he is an x - 3 - 1 - 4 - 5 - 1 / 2 - 4 / 1 - 1, and a - 1 ' s - - - 0 - 2 - 6 - - 1? - - 7 - - 4\",\n",
       "  \"this is a picture of a calculator with the word dx written on it and a person ' s hand holding a pen in front of it, pointing at the sign that says dx = x = i - x = x - i - y - u - v - 1 - x - o - 2 - 3 - 4 - 0 - 1 / c - 1 + - 1? - 2c - - - b - 1s - 1, - 2, and a - 2\",\n",
       "  \"this is a picture of a calculator with the word dx written on it and a person ' s hand holding a pen in front of it, pointing at the sign that says dx = x = i - x = x - i - y - u - v - 1 - x - o - 2 - 3 - 4 - 0 - 1 / c - 1 + - 1? - 2c - - - b - 1s - 1, - 2, and a - 2\"])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caption_list), caption_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb8ee231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb, met, ids = detect_scenes(video_path=\"../This Integral Breaks Math.mp4\", scene_list= scene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc865e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path = \"../db_path\")\n",
    "collection = client.get_or_create_collection(\"frame_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd132815",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    ids = ids,\n",
    "    embeddings= emb,\n",
    "    metadatas= met,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c1d9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(00:00:00.000 [frame=0, fps=30.000], 00:00:01.233 [frame=37, fps=30.000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ca5dc87",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.\n401 Client Error. (Request ID: Root=1-6988df00-0c02343335eec9b9385a4499;82209a26-925c-4ae7-b207-ab86528e1a8e)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\nAccess to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:657\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '401 Unauthorized' for url 'https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:419\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    418\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001b[34m__name__\u001b[39m, kwargs=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1032\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, tqdm_class, dry_run)\u001b[39m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1032\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1046\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1183\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, tqdm_class, dry_run)\u001b[39m\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m         \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1812\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1807\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1808\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1809\u001b[39m ):\n\u001b[32m   1810\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1814\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1699\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder, retry_on_errors)\u001b[39m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1699\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_on_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_on_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RemoteEntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001b[34m__name__\u001b[39m, kwargs=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1622\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, timeout, library_name, library_version, user_agent, headers, endpoint, retry_on_errors)\u001b[39m\n\u001b[32m   1621\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1622\u001b[39m response = \u001b[43m_httpx_follow_relative_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_on_errors\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m hf_raise_for_status(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:309\u001b[39m, in \u001b[36m_httpx_follow_relative_redirects\u001b[39m\u001b[34m(method, url, retry_on_errors, **httpx_kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m response = http_backoff(\n\u001b[32m    303\u001b[39m     method=method,\n\u001b[32m    304\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     **no_retry_kwargs,\n\u001b[32m    308\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# Check if response is a relative redirect\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:677\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    674\u001b[39m     message = (\n\u001b[32m    675\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    676\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mGatedRepoError\u001b[39m: 401 Client Error. (Request ID: Root=1-6988df00-0c02343335eec9b9385a4499;82209a26-925c-4ae7-b207-ab86528e1a8e)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\nAccess to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor, AutoModelForImageTextToText\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m processor = \u001b[43mAutoProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/gemma-3-4b-it\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m model = AutoModelForImageTextToText.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgoogle/gemma-3-4b-it\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m messages = [\n\u001b[32m      7\u001b[39m     {\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     },\n\u001b[32m     14\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py:362\u001b[39m, in \u001b[36mAutoProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PreTrainedConfig):\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m     processor_class = \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mprocessor_class\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoProcessor\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1376\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1373\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1374\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m config_dict, unused_kwargs = \u001b[43mPreTrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1378\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:569\u001b[39m, in \u001b[36mPreTrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:624\u001b[39m, in \u001b[36mPreTrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    623\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    638\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:276\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    222\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m | os.PathLike,\n\u001b[32m    223\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    224\u001b[39m     **kwargs,\n\u001b[32m    225\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    226\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    228\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    274\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fareh\\coding\\video-rag\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:483\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure to have access to it at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    486\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, LocalEntryNotFoundError):\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001b[31mOSError\u001b[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-3-4b-it.\n401 Client Error. (Request ID: Root=1-6988df00-0c02343335eec9b9385a4499;82209a26-925c-4ae7-b207-ab86528e1a8e)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-3-4b-it/resolve/main/config.json.\nAccess to model google/gemma-3-4b-it is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeef4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-rag (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
